# Part 2. 분산 데이터

## 분산된 DB를 요구하는 이유

1. 확장성 : R/W 부하가 커지면 여러 장비로 분산 가능

2. 내결함성/고가용성 : 장비 하나가 죽어도 다른 장비에서 작업 지속 가능

3. 지연시간 : 글로벌하게 유저가 있다면, 유저와 가장 가까운 지역에 서버에서 응답 가능

## 고부하로 확장

- 고부하 확장이 필요하면 더 강력한 스펙의 장비 구매해서 대응 (수직 확장 또는 용량 확장)

- 공유 메모리 아키텍처 : 많은 CPU, RAM, DISK를 하나의 운영체제에서 운영 가능
  - 문제점
    - 비용이 Linear하게 증가함.
    - 병목 현상으로 스펙을 2배 늘린다고, 2배의 부하를 처리할 수 있는 것이 보장되지 않음.
- 비공유 아키텍처 (수평 확장 또는 규모 확장) -> **2부의 초점**
  - DB 소프트웨어를 실행하는 각 장비 또는 가상 장비를 노드라 부름.
  - CPU, RAM, DISK를 독립적으로 사용.
- 여러 노드에 데이터를 분산하는 방법
  - 복제: 같은 데이터의 복제를 여러 노드에 유지
  - 파티셔닝: 큰 DB를 작은 서브셋 데이터로 쪼개서 각 파티션을 여러 노드에 배치 (샤딩이라고도 함)

# Chapter 5. 복제

- 복제를 수행함으로써 데이터를 사용자와 가깝게 유지해 Latency를 낮춤.
- 서버에 장애가 있어도, 작업 지속 가능 (가용성 높임)
- 읽기 질의 처리하는 장비의 수를 늘려 읽기 처리량 향상

- 복제의 어려움 : 데이터에 변경이 있을 때, 어떻게 처리해 줄 것인가?
  - 단일 리더 (Single-Leader)
  - 다중 리더 (Multi-Leader)
  - 리더 없음 (Leaderless)

## 5.1 리더와 팔로워

- DB 복사본을 저장하는 각 노드를 복제 (replica) 라고 함
- DB의 모든 쓰기는 복제 서버에서 처리되어야함.

  - 리더 기반 복제 (능/수동, 마스터-슬레이브 복제)
  - 리더가 로컬 저장소에 새로운 데이터를 기록할 때마다 데이터 변경을 복제 로그 또는 변경 스트림의 일부로 팔로워 (슬레이브, 읽기 복제서버, 2차, 핫 대기)에게 전송.

- 클라이언트가 읽기 요청을 하면 마스터 또는 슬레이브가 처리 가능하지만, 쓰기는 마스터만 가능

- DB 뿐만 아니라 인메모리 DB, 분산 메모리 브로커 등에도 적용가능함 (카프카, RabbitMQ, Redis)

## 5.2 동기식 vs 비동기식 복제

- 동기식 : 작업 하나가 끝날 때까지 나머지 작업은 대기.

  - 장점은 팔로워와 리더가 일관되게 최신 데이터 복사본을 갖고 있게 하는 것을 보장.
  - 단점은 작업이 마무리 되었다는 응답이 있기 전까지 나머지 작업들은 대기해야함.
  - 현실적으로 동기식 복제는 팔로워 하나는 동기식으로, 나머지는 비동기식으로 처리해야함. (반동기식)

- 비동기식 : 보통 리더 기반 복제는 비동기식.
  - 장점은 모든 팔로워가 잘못 되더라도 리더는 쓰기 지속 가능.
  - 내구성을 약화시키는 것 같지만, 데이터센터가 분산되어 있으면 비동기식 복제 널리 사용.

## 5.3 새로운 팔로워 설정

- 새로운 팔로워가 리더 데이터 복제본을 정확히 갖고 있는지 보장하는 방법

  - DB Lock이 아닌 스냅샷 사용 -> 최신 스토리지 서버들은 기본 제공 (ZFS)
  - 스냅샷은 팔로워 노드에 복사
  - 스냅샷 이후 발생한 데이터에 대해 변경 요청
  - 복제 노드에서 스냅샷 이후 변경 사항 반영했다 응답하면 -> 마스터 노드 신규 데이터 처리
  - 최신으로 출시되는 고가용성 데이터 서버는 거의 대부분 ZFS 파일 시스템 차용하면서 스냅샷 기본 제공
  - 최신 Workflow Pipeline에서 실행되는 스케줄러들도 스케줄링 서버가 죽으면 죽은 시점에서부터 실행되어야하는 스케줄 작업 전부 실행함. ([Flyte Scheduler CatchupAll-system](https://docs.flyte.org/en/latest/concepts/component_architecture/native_scheduler_architecture.html#catchupall-system))

## 5.4 리더의 장애 복구 (Failover)

1. 리더가 장애인지 판단함.

2. 새로운 리더 선택.

- Redis-Sentinel을 사용하면 Quorum 구조

- 참고: [redis-sentinel](http://redisgate.jp/redis/sentinel/sentinel.php)

3. 새로운 리더 사용을 위해 시스템 재설정

- 시스템에서 이전 리더가 팔로워가 되고, 새로운 리더 인식할 수 있게 변경함.

4. 장애 복구 과정은 잘못될 수 있는 것 투성이임.

- 리더 선출되기 전에 쓰기 요청이 들어오면? 가장 간단한 방법은 해당 쓰기 요청을 drop -> 하지만 내구성 원칙을 버리는 일이됌.

- 쓰기를 폐기하는 방법은 데이터베이스 외부의 다른 저장소 시스템이 데이터베이스 내용에 맞춰 조정되는 상황에 특히 위험함.

- 특정 결함 상황에서 두 개 이상의 노드가 자신이 리더라고 믿는 상황은 무척 위험 (스플릿 브레인, Split Brain)

  - 쓰기 요청을 받으면 충돌이 발생할 확률이 무척 높은데, 충돌 해소를 안하면 데이터 유실 또는 오염 발생.

- 리더가 죽었다고 판단되는 적절한 타임아웃은? 시스템마다 다를듯.
  - 너무 길면 복구까지 시간이 길어짐 (데이터 유실 확률도 높음)
  - 너무 짧으면 불필요한 장애 복구 발생 (마스터 퇴출 / 새 리더 선출이 빈번해질 수도 있다?)

## 5.5 리더 기반 복제

- 리더 기반 복제의 내부

### 5.5.1 구문 기반 복제

- 모든 쓰기 요청을 (구문, statement) 기록하고 쓰기 실행 이후 다음 구문 로그를 팔로워에게 전송.

- RDBS에서는 클라이언트에게 직접 요청을 받은 것처럼 모든 Insert, Update, Delete 구문을 파싱하고 실행.

  - 문제점.
    - NOW() 나 RAND()와 같이 비결정적 함수를 호출하는 작업은 각 서버마다 다른 값을 생성할 가능성이 무척 높음.
    - 자동 증가 칼럼 또는 DB에 의존하는 데이터 사용 시, 구문은 각 복제 데이터에서 정확히 같은 순서로 실행되어야함. 이 방식은 여러 트랜잭션 실행을 제한하는 행위.
    - 부수 효과를 가진 구문 (스토어드 프로시저, 트리거, 사용자 정의 함수)은 부수효과가 완벽하게 결정적이지 않으면 각 복제 서버에서 다른 부수 효과가 발생함.

### 5.5.2 쓰기 전 로그 배송

- 일반적으로 모든 쓰기는 로그에 기록됨.

  - 로그 구조화 저장소 엔진 (2장의 SS 테이블, LSM 트리 참고) 로그 자체가 저장소의 주요 부분. 로그 세그먼트를 작게 유지하고, 백그라운드에서 GC 수행
  - 개별 디스크 블록에 덮어 쓰는 B 트리의 모든 변경은 쓰기 전 로그 (WAL) 에 기록되기 때문에 고장 이후에 일관성 있는 로그로 기록 가능.

- 위의 2가지 케이스는 모두 DB의 쓰기를 포함하는 Append-Only 바이트열임.
- 팔로워가 이 로그를 처리하면 정확히 동일한 데이터 구조의 복제본을 만드는 것이 가능함.
  - Postgresql & Oracle에서 사용.
- 가장 큰 단점은 로그가 제일 저수준의 데이터를 기술한다는 점.
  - WAL이 어떤 디스크 블록에서 어떤 바이트를 변경했다는 정보를 포함함으로써 복제가 저장소 엔진과 밀접하게 연결됨.
  - 결과적으로 데이터베이스 저장소 형식을 다른 버전으로 변경 시, 대개 리더와 팔로워의 데이터베이스 소프트웨어 버전을 다르게 실행할 수 없음.

### 5.5.3 논리 기반 로그 복제

- 복제와 저장소 엔진의 분리를 위한 방법 중 하나는 다른 로그 형식을 사용하는 것. -> 즉, 논리 로그

- RDBMS에서 논리 로그는 대개 로우 단위로 DB Write를 기술한 코드 열임.

  - 삽입된 Row Log는 모든 Column의 새로운 값 포함
  - 삭제된 Row Log는 고유하게 식별하는데 필요한 정보 포함. 보통 기본 키를 사용하나 없으면 모든 Column의 모든 Row 조회
  - 갱신된 Row Log는 Row를 고유하게 식별하는데 필요한 정보와 모든 Column의 새로운 값 포함함.

- 논리적 로그를 분리함으로써 특정 버전에 종속되지 않는 하위 호환성을 유지할 수 있고, 다른 버전의 DB 또는 다른 저장소 엔진에서 실행 가능.

- 외부 애플리케이션에서 파싱하기도 쉽고, 변경 데이터 캡처 (Change Data Capture)라고 부름 (11장에서 추가 설명 예정)

### 5.5.4 트리거 기반 복제

- 사용자 정의 애플리케이션 코드 등록. 데이터 변경 이벤트 발생 시, 자동 실행.
- 다른 복제 방식보다 오버헤드가 존재하고, 버그나 제한 사항이 더 많이 발생.

## 5.6 복제 지연 문제

- 데이터를 복제하는 것은 내결함성 뿐만 아니라 확장성, 지연시간을 처리하는 것이 또다른 이유임.
- 여러 복제 노드를 추가함으로써 읽기 확장을 할 수 있지만, 이는 비동기식 복제에서만 동작함.

  - 동기식으로 모든 팔로워 복제를 시도할 경우, 단일 노드 장애 또는 네트워크 중단으로 전체 시스템 쓰기 불가함.

- 문제는 팔로워가 데이터 반영이 늦어지면 과거 데이터를 볼 수 있는 점임.
  - 일반적으로 복제 서버가 얼마나 뒤처질지는 상황에 따라 다르고, 시스템 리소스 가용량 인근 또는 네트워크 지연이 발생하면 지연 시간은 더 커짐.

### 5.6.1 자신이 쓴 내용 읽기

- 사용자가 데이터를 제출하면 리더에 보내야하지만, 사용자가 데이터를 볼 때는 팔로워에서 읽게함.
- 자주 읽지만 가끔 쓰는 작업에 적합함.
- 사용자가 쓰기를 수행한 직후 데이터를 보고 싶을 때, 반영이 느리면 사용자 입장에서 불만족스러움.
- 쓰기 후 읽기 일관성 (또는 자신의 쓰기 일관성)
- 구현 방법

  - 사용자가 수정한 내용을 읽을 때 리더에서 수행.
  - 애플리케이션 내 대부분 내용을 사용자가 편집할 가능성이 높으면, 대부분 리더에서 읽기 때문에 적합하지 않음.
    - 이 경우, 마지막 갱신 시간을 걸어서 일정 시간이 지나야 리더에서 읽게 하는 등의 조건을 걸어둘 수 있음.
  - 클라이언트에서 가장 최신 쓰기 타임 스탬프를 기억해서 사용자 읽기를 위한 복제 서버가 최소한 해당 타임 스탬프까지 갱신을 반영하게 할 수 있다.
  - 복제 서버가 여러 데이터 센터에 분산되었다면 복잡도가 증가하므로, 리더가 제공하는 모든 요청은 리더가 포함된 데이터 센터로 라우팅 되어야함.

- 동일한 사용자가 여러 디바이스 (웹 브라우저 & 모바일 앱)로 접근할 때 또다른 문제 발생.

  - 디바이스간 쓰기 후 읽기 일관성 필요함.
  - 사용자가 한 디바이스에서 입력을 하면, 다른 디바이스에서 이 입력한 값이 보여야함. (ATM에서 돈을 입출금을 하면, 모바일 앱에서도 동일하게 보여야함)

  - 이 때 추가 고려 사항 발생
    - 사용자의 마지막 타임 스탬프까지 고려하는 것은 무척 어려움. 이 메타 데이터는 중앙 집중식으로 관리 필요.
    - 복제 서버가 여러 데이터 센터에 분산 되어 있는 상황에서, 사용자 요청이 동일한 데이터 센터로 라우팅 된다는 보장 없음. 리더에서 읽어야할 필요가 있는 접근법이라면 먼저 사용자 디바이스 요청을 동일 데이터 센터로 라우팅해 줄 수 있도록 해야함.

### 5.6.2 단조 읽기

- 비동기 읽기에서 발생할 수 있는 두번째 이상은 사용자가 시간이 거꾸로 흐르는 현상을 목격할 수 있음.

- 예를 들어, 사용자2345가 동일한 쿼리를 2번 보냈을 때 첫번재 쿼리는 최신 반영 데이터를 반환하나, 두번째 쿼리에서 아무것도 반환 안하는 케이스임. (즉, 2번째 보낸 쿼리가 더 최신인데, 업데이트 된 데이터를 못 가져오는 경우)

- 단조 읽기는 위의 예시를 발생하지 않게 보장해주는 행위임.

- 강한 일관성 >> 단조 읽기 >> 최종적 일관성

- 한 사용자가 여러 번에 걸쳐 여러번 읽어도, 과거 데이터를 읽어들이지 않음.

- 단조 읽기의 달성 방법 중 한 가지는 각 사용자의 읽기가 항상 동일한 복제 서버에서 수행하게끔 하는 방법임.

### 5.6.3 일관된 순서로 읽기

- 세번째 복제 지연 이상 현상은 인과성의 위반 우려.

- 일련의 쓰기가 특정 순서로 발생한다면, 이 쓰기를 읽는 모든 사용자는 같은 순서로 쓰여진 내용을 보게 됨을 보장함.

- 일관된 순서로 읽기가 보장 안되는 경우는 파티셔닝 된 DB에서 특징적인 문제임.

  - 서로 다른 파티션은 독립적으로 수행되므로 쓰기의 전역 순서는 없음.
  - 문제의 한 가지 해결 방법은 동일한 파티션에서 DB를 읽게하는 방법

- 복제 지연을 위한 해결책으로 트랜잭션이 존재하는 이유: "올바른 작업 수행"을 위해 데이터베이스를 신뢰할 수 있게 하기 때문.

## 5.7 다중 리더 복제

- 리더 기반 복제의 문제점은 쓰기가 항상 단일 리더를 거쳐야함.

- 쓰기를 처리하는 각 노드는 데이터 변경을 다른 모든 노드에 전달하는데, 이를 다중 리더 복제라함.

- 단일 데이터센터에서 사용하기에는 적합하지 않으나, 2개 이상의 데이터 센터 운영 시에 이점이 큼.

- 단순하게 생각해보면, 리더 노드가 여러 데이터센터에 거쳐 하나만 존재한다면, 쓰기 지연이 발생할 수 밖에 없음.

- 무엇보다 여러 데이터센터를 갖고 있음으로써 사용자에게 최대한 가까이 데이터를 두는 목적에도 위배됨.

- 다중 리더 설정에서는 각 데이터 센터는 독립적으로 운영되고, 고장난 데이터 센터가 온라인으로 복구되면 복제 데이터를 따라 잡음.

- `네트워크 이슈` 등에서도 잘 버팀. 일시 네트워크 중단에도 쓰기 작업은 다른 데이터 센터에서 진행되기 때문.

- `오프라인 클라이언트 작업`에도 무척 효과적임.

  - 모바일, 태블릿, 노트북 등이 오프라인에서 작업한 것이 인터넷 연결 시, 복제 서버와 동기화.
  - 이는 디바이스에 로컬 DB가 있고, 원격 서버와 비동기 동기화가 되는 방식임.
  - 아키텍처 관점으로 보면 근본적으로 다중 데이터 센터간 다중 리더 복제와 동일함.

- `협업 편집`

  - 구글 독스 등에서 사용하는 실시간 협업 편집
  - 앞서 언급한 `오프라인 클라이언트 작업`과 공통점이 많음.
  - 한 사용자가 문서 편집할 때 변경 내용을 즉시 로컬에 반영 후, 동일한 문서를 편집하는 다른 사용자와 서버에 비동기 식으로 복제.
  - 편집 충돌 없음을 보장하려면 한 사용자가 작업이 커밋된 후, 잠금이 해제될 때까지 기다리는 방식임.
  - 더 빠른 협업을 위해 단위를 작게 나눠 문제를 피할 수 있는데, 여러 사용자가 동시에 편집할 수 있으나 충돌 해소를 포함해 리더 복제에 발생하는 모든 문제 야기함.

### 5.7.1 쓰기 충돌 다루기

#### 5.7.1.1 동기 대 비동기 쓰기 충돌 다루기

- 첫번재 쓰기가 마무리 될 때까지 두번째 쓰기 또는 두번째 트랜잭션을 차단 후, 재시도

#### 5.7.1.2 충돌 회피

- 특정 레코드의 모든 쓰기가 동일한 리더를 거치도록 보장하면 애플리케이션 충돌은 발생하지 않음.

- 하지만 특정 데이터센터가 고장났을 때 트래픽을 다른 데이터 센터로 라우팅하거나 사용자가 다른 데이터센터에 더 가까이 있는 경우 충돌 회피는 실패함.

#### 5.7.1.3 일관된 상태 수렴

- 모든 복제 계획은 모든 복제 서버가 최종적으로 동일해야한다는 사실을 보장해야함.

- 따라서 DB는 수렴 (Convergent) 방식으로 충돌 회피를 해야함.

- 즉, 모든 변경이 복제돼 모든 복제 서버에 동일한 최종 값이 전달되게 해야한다는 의미임.

- 적용 예시

  - 각 쓰기에 고유 ID를 부여해 가장 높은 ID를 가진 쓰기를 고른다.
  - 각 복제 서버에 고유 ID를 부여하고 높은 숫자의 고유 ID를 가진 서버에서 생긴 쓰기를 낮은 숫자의 고유 ID를 가진 서버에서 생긴 쓰기보다 항상 우선되게 적용하는 방법. 데이터 유실 이슈 존재함.
  - 어떻게든 값 병합.
  - 명시적으로 값 충돌을 기록해 모든 정보 보존. -> Git?

### 5.7.2 사용자 정의 충돌 해소 로직

- 충돌 해소 로직은 앱마다 다름. 대부분의 다중 리더 복제는 애플리케이션 코드를 사용해 충돌 해소 로직 작성.

- 쓰기 수행 중에는 충돌 핸들러 로직 호출해서 작업하고, 사용자에게 충돌을 보여주기보다 백그라운드에서 실행

- 읽기 수행 중에는 충돌 감지 시, 모든 충돌 쓰기 저장. 다음 번 데이터 읽을 때 이런 여러 버전의 데이터가 애플리케이션 상에 반한됨.

  - 사용자 (수동) 또는 애플리케이션에 의해 (자동)으로 충돌해결하면 결과는 다시 DB에 기록.

- 자동 충돌 해소 알고리즘 예시

  - 충돌 없는 데이터 타입
  - 병합 가능한 영속 데이터 구조
  - 운영 변환

#### 5.7.2.1 충돌의 정의

- 동일 레코드의 동일 필드를 동시에 수정해 2개의 서로 다른 값으로 지정

### 5.7.3 다중 리더 복제 토폴로지

- 복제 토폴로지는 쓰기를 한 노드에서 다른 노드로 전달하는 통신 경로를 설명하는 행위.

![copy topology](https://github.com/ttlqudan/WoowahanStudy/assets/40455392/75ddeb0a-1335-4eb2-a538-b7cf5a912917)

- 가장 일반적인 연결은 전체 연결 (all-to-all, 그림의 C에 해당)
- 각 노드가 하나의 쓰기를 받고 이를 전달하는 행위의 순환은 원형 토폴로지 (Circular Topology, 그림의 A에 해당)
- 지정된 루트 노드 하나가 다른 모든 쓰기에 전달하는 행위는 별형 토폴로지라고함 (Star Topology, 그림의 B에 해당)
- 원 & 별형 토폴로지는 하나의 노드에서 문제가 발생하면 다른 노드간 복제 행위에 영향을 줌.
  - 단일 장애점 (Single Point of Failure)
- 전체 연결 토폴로지가 상대적으로 좋아보이지만, 문제는 특정 노드의 속도가 다른 노드의 속도를 추월할 수 있음.
  - 이런 이벤트 정렬을 위해 버전 벡터 사용.

## 5.8 리더 없는 복제

- 리더 시스템을 버리고 복제 서버가 클라이언트로부터 직접 쓰기를 받을 수 있게 허용.
- AWS Dynamo DB가 대표 예시이며, Dynamo Style이라고 함.
- 일부 Leaderless 구현 방식에서는 클라이언트가 복제 노드에 직접 각 각 요청을 쏴주며 처리하기도 하지만, 코디네이터 노드를 두고 이럴 처리하는 구현도 존재함. 단, 코디네이터 노드는 특정 순서 쓰기 지원이 안됌.

### 5.8.1 노드가 다운 되었을 때 데이터베이스 쓰기.
