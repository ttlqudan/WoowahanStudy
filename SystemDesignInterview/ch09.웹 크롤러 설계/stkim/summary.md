# Ch09. 웹 크롤러 설계

- 크롤링 사용 목적

  - 검색 엔진 인덱싱 : 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스 생성.

  - 웹 아카이빙 : 나중에 사용할 목적으로 장기 보관하기 위해 웹에서 정보 모으는 절차

  - 웹 마이닝 : 유용한 지식 도출

  - 웹 모니터링 : 크롤러를 사용하면 인터넷에서 저작권이나 상표권 침해되는 사례 모니터링 가능.

## 1. 문제 이해 및 설계 범위 확정

- 검색 엔진 인덱싱에 사용

- 10억 (1Billion) 웹 페이지 수집

- 새로 만들어진 웹페이지나 수정된 웹 페이지도 고려

- 수집한 웹 페이지는 5년간 저장이 필요함.

- 중복된 콘텐츠를 갖는 페이지는 무시해도 무방함.

- 그 외 웹 크롤러를 구성하기 위해 필요한 속성들

  - 확장성 : 병행성을 사용해서 효과적인 웹 크롤링 수행

  - 안정성 : 잘못 작성된 HTML, 아무 반응이 없는 서버, 장애, 악성 코드가 붙어 있는 링크 등에 잘 대응

  - 예절 : 짧은 시간 동안 너무 많은 요청 보내면 안됨

  - 확장성 : 새로운 형태 콘텐츠 지원이 용이해야함.

### 개략적 규묘 추정

- 매달 10억개 웹 페이지 다운로드

- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400페이지/초

- 최대 QPS = 2 \* QPS = 800

- 웹 페이지 최대 크기 평균은 500k

- 10억 페이지 x 500k = 500TB/Month

- 5년 간 저장하면 30PB 필요 (500TB x 12Month x 5years)

## 2. 개략적 설계안 제시 및 동의 구하기

### 시작 url 집합

- 크롤러 시작 지점.

- 크롤러가 가능한 많은 링크 탐색할 수 있는 url 선택 필요

- 일반적으로 전체 url 공간을 작은 부분집합으로 나누는 전략 필요.

### 미수집 url 저장소

- 현대 웹 크롤러의 상태는 2가지로 나눠 관리함.

  - 다운로드할 url. 또는 미수집 url 저장소라고도 부름

  - 다운로드 된 url

### HTML 다운로더

- 인터넷 웹페이지를 다운로드하는 컴포넌트

### 도메인 이름 변환기

- 웹 페이지를 다운로드 받으려면 url을 ip 주소로 변환하는 절차가 필요함.

### 콘텐츠 파서

- 콘텐츠를 다운로드하면 Parsing과 Validation이 필요함.

- 이상한 웹 페이지는 문제 발생할 여지가 다분하고, 저장 공간 낭비 가능성이 높음.

### 중복 콘텐츠인가?

- 웹의 29% (편의상 30%)는 중복 컨텐츠임

- 자료 구조를 도입해서 데이터 중복을 줄이고, 데이터 처리에 소요되는 시간을 줄인다.

- 효과적인 방법으로 해시 값 비교가 있음.

### 콘텐츠 저장소

- 저장 데이터 유형, 크기, 저장소 접근 빈도, 데이터 유효 기간 등을 종합적으로 고려함.

- 데이터 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장

- 인기 있는 컨텐츠는 메모리에 둬서 접근 지연시간을 줄인다.

### URL 추출기

- 상대 경로는 전부 origin url(예를 들어 https://en.wikipedia.org)를 붙여 절대 경로로 변환.

### URL 필터

- 특정 콘텐츠 타입이나 확장자, 접속 시 오류가 발생하는 url, 접근 제외 목록에 포함된 url 등을 크롤링 대상에서 배제하는 역할

### 이미 방문한 URL?

- 이미 방문한 url이나 미수집 url 저장소에 보관된 url을 추적할 수 있도록 하는 자료 구조 사용.

- 같은 url을 여러 번 처리하는 일을 방지 가능.

- 자료 구조로 해시 테이블이나 블룸 필터가 널리 사용됨.

### URL 저장소

- 이미 방문한 url을 보관하는 저장소.

### 웹 크롤러 작업 흐름

1. 시작 url들을 미수집 url 저장소에 저장.

2. HTML 다운로더는 미수집 url 저장소에서 url 목록을 가져온다.

3. HTML 다운로더는 도메인 이름 변환기를 사용해서 URL의 ip 주소를 알아내고, 해당 ip 주소로 접속하여 웹 페이지를 다운로드 받음.

4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.

5. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시한다.

6. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.

   - 이미 저장소에 있는 컨텐츠인 경우에는 처리하지 않고 버린다.

   - 저장소에 없는 컨텐츠인 경우 저장소에 저장한 뒤 url 추출기로 전달.

7. url 추출기는 해당 HTML 페이지에서 링크를 골라낸다.

8. 골라낸 링크를 url 필터로 전달한다.

9. 필터링이 끝나고 남은 url만 중복 url 판별 단계로 전달.

10. 이미 처리한 url인지 확인하기 위해, url 저장소에 보관된 url인지 살핀다. 이미 저장소에 있는 url은 버린다.

11. 저장소에 없는 url은 url 저장소에 저장할 뿐 아니라 미수집 url 저장소에도 전달한다.

## 3. 상세 설계

- 가장 중요한 컴포넌트와 그 구현 기술

  - DFS vs BFS

  - 미수집 url 저장소

  - HTML 다운로더

  - 안정성 확보 전략

  - 확장성 확보 전략

  - 문제 있는 콘텐츠 감지 및 회피 전략

### DFS vs BFS

- 그래프가 어느 정도 이상 크면 DFS로 얼마나 깊이 있게 탐색할지 가늠이 어려움.

- BFS를 그래서 보통 많이 사용. FIFO 기반 큐.

  - 단, 2가지 문제 존재함

    - 같은 호스트의 많은 링크 다운받느라 바빠져, 이 때 링크들을 병렬로 처리하게 된다면 위키피디아 서버는 수많은 요청으로 과부하 걸리게 됨.

    - 모든 웹페이지가 같은 수준의 품질, 같은 수준의 중요성 안가지므로 페이지 순위 (page rank), 사용자 트래픽, 업데이트 빈도 등 여러 척도에 비추어 처리 우선순위를 구별하는 게 온당함.

### 미수집 URL 저장소

- 작성 중
